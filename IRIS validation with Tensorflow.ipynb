{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "R6_InternalLab_AIML_Prakruth.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "YJRBuqXhOB7_",
        "_4bLlfQmLO6V",
        "3GlbpGYELO6Z",
        "70LKIEm3LO6b",
        "10tHGSsGLO6h"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sb7Epo0VOB58"
      },
      "source": [
        "### Load tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fHpCNRv1OB5-",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwC-cVT2L9nc",
        "colab_type": "code",
        "outputId": "324b7f16-84e1-4e6c-ce73-7951efce0e15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.14.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeYBYIOpLO4S",
        "colab_type": "text"
      },
      "source": [
        "### Enable Eager Execution if you are using tensorflow 1.x"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tnSsH8sNOB6F",
        "colab": {}
      },
      "source": [
        "##Enable Eager Execution\n",
        "tf.enable_eager_execution()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DxJDmJqqOB6K"
      },
      "source": [
        "### Collect Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FhllFLyKOB6N",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "B4yQKMiJOB6R",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv('./prices.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fgkX6SEqOB6W"
      },
      "source": [
        "### Check all columns in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7K8pWsNQOB6X",
        "outputId": "605c7db5-44f2-4a69-83d6-11f3197414b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data.columns"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['date', 'symbol', 'open', 'close', 'low', 'high', 'volume'], dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7dU6X7MpOB6c"
      },
      "source": [
        "### Drop columns `date` and  `symbol`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lh_6spSKOB6e",
        "colab": {}
      },
      "source": [
        "data = data.drop(columns = ['date','symbol'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xlwbUgTwOB6i",
        "outputId": "3da7f028-835d-4fd6-a8a0-030150ad712b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        }
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>open</th>\n",
              "      <th>close</th>\n",
              "      <th>low</th>\n",
              "      <th>high</th>\n",
              "      <th>volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>123.430000</td>\n",
              "      <td>125.839996</td>\n",
              "      <td>122.309998</td>\n",
              "      <td>126.250000</td>\n",
              "      <td>2163600.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>125.239998</td>\n",
              "      <td>119.980003</td>\n",
              "      <td>119.940002</td>\n",
              "      <td>125.540001</td>\n",
              "      <td>2386400.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>116.379997</td>\n",
              "      <td>114.949997</td>\n",
              "      <td>114.930000</td>\n",
              "      <td>119.739998</td>\n",
              "      <td>2489500.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>115.480003</td>\n",
              "      <td>116.620003</td>\n",
              "      <td>113.500000</td>\n",
              "      <td>117.440002</td>\n",
              "      <td>2006300.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>117.010002</td>\n",
              "      <td>114.970001</td>\n",
              "      <td>114.089996</td>\n",
              "      <td>117.330002</td>\n",
              "      <td>1408600.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         open       close         low        high     volume\n",
              "0  123.430000  125.839996  122.309998  126.250000  2163600.0\n",
              "1  125.239998  119.980003  119.940002  125.540001  2386400.0\n",
              "2  116.379997  114.949997  114.930000  119.739998  2489500.0\n",
              "3  115.480003  116.620003  113.500000  117.440002  2006300.0\n",
              "4  117.010002  114.970001  114.089996  117.330002  1408600.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3DBv3WWYOB6q"
      },
      "source": [
        "### Consider only first 1000 rows in the dataset for building feature set and target set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Z_hG9rGBOB6s",
        "colab": {}
      },
      "source": [
        "data_new = data.head(1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMOkGKc1LO5C",
        "colab_type": "text"
      },
      "source": [
        "### Convert Float64 to Float32"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Vn1H1uwLO5E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_new = data_new.astype('float32')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYpIhPoPQRLk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = data_new.drop(columns='close')\n",
        "Y = data_new.close"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBwwz33mW4bG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlNPyphoQPyR",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "M3UaApqYOB6x"
      },
      "source": [
        "### Divide the data into train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtaHe7gxPyPh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2EkKAy7fOB6y",
        "colab": {}
      },
      "source": [
        "X_train,X_test,y_train,y_test= train_test_split(X,Y,test_size =0.2,random_state = 123)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yg8tMvKVZKyj",
        "colab_type": "code",
        "outputId": "90740087-b56e-42c8-8653-ea491d7aab52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(800, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-N2U6kbLO5N",
        "colab_type": "text"
      },
      "source": [
        "### Normalize Train and Test Data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElpbKjAOLO5Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import Normalizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAHL_XwsUhRO",
        "colab_type": "code",
        "outputId": "15dbbca6-343a-4207-a2a0-4e89579337b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155
        }
      },
      "source": [
        "train_x"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2.2876651e-05, 2.2689075e-05, 2.3034214e-05, 1.0000000e+00],\n",
              "       [4.7349291e-05, 4.6249999e-05, 4.7349291e-05, 1.0000000e+00],\n",
              "       [7.6225746e-07, 7.5391358e-07, 7.7544627e-07, 1.0000000e+00],\n",
              "       ...,\n",
              "       [2.3780945e-05, 2.3505878e-05, 2.4289406e-05, 1.0000000e+00],\n",
              "       [1.7972880e-05, 1.7853366e-05, 1.8119972e-05, 1.0000000e+00],\n",
              "       [3.1503550e-05, 3.1358217e-05, 3.1951997e-05, 1.0000000e+00]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lgoDlbLZQPz",
        "colab_type": "code",
        "outputId": "2fc1eedc-1672-4b9a-ec4d-3db8139ed490",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "y_train"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "512     30.350000\n",
              "685     52.730000\n",
              "997     28.770000\n",
              "927     14.940000\n",
              "376     26.790001\n",
              "788     23.209997\n",
              "91     123.910004\n",
              "170    128.550003\n",
              "584     25.000000\n",
              "90     122.970001\n",
              "75     123.790001\n",
              "747     69.430000\n",
              "969     37.380001\n",
              "368     56.060001\n",
              "893     98.089996\n",
              "529     27.430000\n",
              "870     25.680000\n",
              "97     126.620003\n",
              "897     66.949997\n",
              "272     22.330000\n",
              "881     10.960000\n",
              "936     37.169998\n",
              "270     29.889999\n",
              "708     18.719999\n",
              "483     23.340000\n",
              "610     27.120001\n",
              "431     59.230000\n",
              "754     37.700001\n",
              "729     66.019997\n",
              "279     38.599998\n",
              "          ...    \n",
              "208    125.900002\n",
              "608     24.629999\n",
              "420     49.430000\n",
              "253     40.380001\n",
              "846     47.560001\n",
              "339     30.500000\n",
              "409     48.880001\n",
              "111    124.959999\n",
              "224    124.820000\n",
              "942     30.350000\n",
              "544     30.950001\n",
              "73     122.309998\n",
              "47     120.629997\n",
              "638     20.440001\n",
              "113    124.839996\n",
              "96     125.949997\n",
              "737     29.330000\n",
              "214    118.790001\n",
              "569     45.579998\n",
              "123    124.309998\n",
              "106    129.250000\n",
              "595     60.599998\n",
              "17     114.470001\n",
              "742     36.779999\n",
              "98     126.459999\n",
              "988     48.150002\n",
              "322     32.529999\n",
              "382     29.100000\n",
              "365     38.959999\n",
              "510     76.849998\n",
              "Name: close, Length: 800, dtype: float32"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tHP9itBPAyx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transformer = Normalizer()\n",
        "train_x = transformer.fit_transform(X_train)\n",
        "test_x = transformer.fit_transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "v6vE4eYCOB62"
      },
      "source": [
        "## Building the graph in tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "297_qja4OB7A"
      },
      "source": [
        "2.Define Weights and Bias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "L205qPeQOB7B",
        "colab": {}
      },
      "source": [
        "#We are initializing weights and Bias with Zero\n",
        "w = tf.zeros(shape=(4,1))\n",
        "b = tf.zeros(shape=(1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HgtWA-UIOB7F"
      },
      "source": [
        "3.Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JveGlx25OB7H",
        "colab": {}
      },
      "source": [
        "def prediction(x, w, b):\n",
        "    \n",
        "    xw_matmul = tf.matmul(x, w)\n",
        "    y = tf.add(xw_matmul, b)\n",
        "    \n",
        "    return y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TL1hIwf_OB7M"
      },
      "source": [
        "4.Loss (Cost) Function [Mean square error]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8VSWPiGXOB7P",
        "colab": {}
      },
      "source": [
        "def loss(y_actual, y_predicted):\n",
        "    \n",
        "    diff = y_actual - y_predicted\n",
        "    sqr = tf.square(diff)\n",
        "    avg = tf.reduce_mean(sqr)\n",
        "    \n",
        "    return avg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jzG85FUlOB7U"
      },
      "source": [
        "5.GradientDescent Optimizer to minimize Loss [GradientDescentOptimizer]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cj802w-3OB7X",
        "colab": {}
      },
      "source": [
        "def train(x, y_actual, w, b, learning_rate=0.01):\n",
        "    \n",
        "    #Record mathematical operations on 'tape' to calculate loss\n",
        "    with tf.GradientTape() as t:\n",
        "        \n",
        "        t.watch([w,b])\n",
        "        \n",
        "        current_prediction = prediction(x, w, b)\n",
        "        current_loss = loss(y_actual, current_prediction)\n",
        "    \n",
        "    #Calculate Gradients for Loss with respect to Weights and Bias\n",
        "    dw, db = t.gradient(current_loss,[w, b])\n",
        "    \n",
        "    #Update Weights and Bias\n",
        "    w = w - learning_rate*dw\n",
        "    b = b - learning_rate*db\n",
        "    \n",
        "    return w, b "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZ4srRkoZa5Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xSypb_u8OB7e"
      },
      "source": [
        "## Execute the Graph for 100 epochs and observe the loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DVvgj7eQOB7f",
        "outputId": "88846141-03bb-4a5c-a1f5-6547c56bafe7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for i in range(100):\n",
        "    \n",
        "    w, b = train(train_x, np.array(y_train), w, b)\n",
        "    print('Current Loss on iteration', i, loss(np.array(y_train), prediction(train_x, w, b)).numpy())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Current Loss on iteration 0 7968.088\n",
            "Current Loss on iteration 1 7639.777\n",
            "Current Loss on iteration 2 7337.2095\n",
            "Current Loss on iteration 3 7058.362\n",
            "Current Loss on iteration 4 6801.3794\n",
            "Current Loss on iteration 5 6564.5425\n",
            "Current Loss on iteration 6 6346.2725\n",
            "Current Loss on iteration 7 6145.112\n",
            "Current Loss on iteration 8 5959.727\n",
            "Current Loss on iteration 9 5788.877\n",
            "Current Loss on iteration 10 5631.4185\n",
            "Current Loss on iteration 11 5486.3057\n",
            "Current Loss on iteration 12 5352.5703\n",
            "Current Loss on iteration 13 5229.322\n",
            "Current Loss on iteration 14 5115.733\n",
            "Current Loss on iteration 15 5011.049\n",
            "Current Loss on iteration 16 4914.574\n",
            "Current Loss on iteration 17 4825.6616\n",
            "Current Loss on iteration 18 4743.72\n",
            "Current Loss on iteration 19 4668.2026\n",
            "Current Loss on iteration 20 4598.608\n",
            "Current Loss on iteration 21 4534.4663\n",
            "Current Loss on iteration 22 4475.356\n",
            "Current Loss on iteration 23 4420.877\n",
            "Current Loss on iteration 24 4370.669\n",
            "Current Loss on iteration 25 4324.4014\n",
            "Current Loss on iteration 26 4281.76\n",
            "Current Loss on iteration 27 4242.4624\n",
            "Current Loss on iteration 28 4206.2407\n",
            "Current Loss on iteration 29 4172.8633\n",
            "Current Loss on iteration 30 4142.1016\n",
            "Current Loss on iteration 31 4113.751\n",
            "Current Loss on iteration 32 4087.624\n",
            "Current Loss on iteration 33 4063.5457\n",
            "Current Loss on iteration 34 4041.3533\n",
            "Current Loss on iteration 35 4020.9023\n",
            "Current Loss on iteration 36 4002.0537\n",
            "Current Loss on iteration 37 3984.6848\n",
            "Current Loss on iteration 38 3968.6753\n",
            "Current Loss on iteration 39 3953.9216\n",
            "Current Loss on iteration 40 3940.3247\n",
            "Current Loss on iteration 41 3927.7944\n",
            "Current Loss on iteration 42 3916.2456\n",
            "Current Loss on iteration 43 3905.6023\n",
            "Current Loss on iteration 44 3895.7937\n",
            "Current Loss on iteration 45 3886.7527\n",
            "Current Loss on iteration 46 3878.4224\n",
            "Current Loss on iteration 47 3870.7456\n",
            "Current Loss on iteration 48 3863.6704\n",
            "Current Loss on iteration 49 3857.1497\n",
            "Current Loss on iteration 50 3851.14\n",
            "Current Loss on iteration 51 3845.6\n",
            "Current Loss on iteration 52 3840.496\n",
            "Current Loss on iteration 53 3835.7913\n",
            "Current Loss on iteration 54 3831.456\n",
            "Current Loss on iteration 55 3827.46\n",
            "Current Loss on iteration 56 3823.7783\n",
            "Current Loss on iteration 57 3820.384\n",
            "Current Loss on iteration 58 3817.2568\n",
            "Current Loss on iteration 59 3814.3752\n",
            "Current Loss on iteration 60 3811.7175\n",
            "Current Loss on iteration 61 3809.2705\n",
            "Current Loss on iteration 62 3807.0144\n",
            "Current Loss on iteration 63 3804.936\n",
            "Current Loss on iteration 64 3803.0193\n",
            "Current Loss on iteration 65 3801.252\n",
            "Current Loss on iteration 66 3799.6255\n",
            "Current Loss on iteration 67 3798.1265\n",
            "Current Loss on iteration 68 3796.7424\n",
            "Current Loss on iteration 69 3795.4705\n",
            "Current Loss on iteration 70 3794.2952\n",
            "Current Loss on iteration 71 3793.2136\n",
            "Current Loss on iteration 72 3792.215\n",
            "Current Loss on iteration 73 3791.296\n",
            "Current Loss on iteration 74 3790.4497\n",
            "Current Loss on iteration 75 3789.6687\n",
            "Current Loss on iteration 76 3788.9487\n",
            "Current Loss on iteration 77 3788.2864\n",
            "Current Loss on iteration 78 3787.6753\n",
            "Current Loss on iteration 79 3787.1128\n",
            "Current Loss on iteration 80 3786.5935\n",
            "Current Loss on iteration 81 3786.116\n",
            "Current Loss on iteration 82 3785.6729\n",
            "Current Loss on iteration 83 3785.268\n",
            "Current Loss on iteration 84 3784.8936\n",
            "Current Loss on iteration 85 3784.548\n",
            "Current Loss on iteration 86 3784.2312\n",
            "Current Loss on iteration 87 3783.9392\n",
            "Current Loss on iteration 88 3783.6672\n",
            "Current Loss on iteration 89 3783.4192\n",
            "Current Loss on iteration 90 3783.1904\n",
            "Current Loss on iteration 91 3782.9775\n",
            "Current Loss on iteration 92 3782.784\n",
            "Current Loss on iteration 93 3782.604\n",
            "Current Loss on iteration 94 3782.4375\n",
            "Current Loss on iteration 95 3782.285\n",
            "Current Loss on iteration 96 3782.1465\n",
            "Current Loss on iteration 97 3782.015\n",
            "Current Loss on iteration 98 3781.8977\n",
            "Current Loss on iteration 99 3781.7864\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9smwOW-1OB7k",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9JuLI6bSOB7n",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DOL2ncA1OB7q"
      },
      "source": [
        "### Get the shapes and values of W and b"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZGvtyTeuOB7r",
        "outputId": "409f34e7-ffc7-464f-bc05-7dc366f9e605",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "#Check Weights and Bias\n",
        "print('Weights:\\n', w.numpy())\n",
        "print('Bias:\\n',b.numpy())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Weights:\n",
            " [[2.5760543e-03]\n",
            " [2.5527244e-03]\n",
            " [2.5969257e-03]\n",
            " [3.3135368e+01]]\n",
            "Bias:\n",
            " [33.13537]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vhDtOv5UOB7x",
        "outputId": "f0b943b7-bc40-45b7-f690-a7ff8f50c3e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def prediction(x, w, b):\n",
        "    \n",
        "    xw_matmul = tf.matmul(x, w)\n",
        "    y = tf.add(xw_matmul, b)\n",
        "    \n",
        "    return y\n",
        "\n",
        "prediction(train_x,w,b)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: id=5131535, shape=(800, 1), dtype=float32, numpy=\n",
              "array([[66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27072 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27016 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27073 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27068 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27073 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27073 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27071 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.2707  ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27073 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.270676],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YJRBuqXhOB7_"
      },
      "source": [
        "### Linear Classification using Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8GoNTWXAOB8C"
      },
      "source": [
        "### Building the simple Neural Network in Keras with one neuron in the dense hidden layer.\n",
        "#### Use Mean square error as loss function and sgd as optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zpeL5rCTOB8D",
        "colab": {}
      },
      "source": [
        "#Initialize Sequential Graph (model)\n",
        "model = tf.keras.Sequential()\n",
        "\n",
        "#Add Dense layer for prediction - Keras declares weights and bias automatically\n",
        "model.add(tf.keras.layers.Dense(1, input_shape=(4,)))\n",
        "\n",
        "#Compile the model - add Loss and Gradient Descent optimizer\n",
        "model.compile(optimizer='sgd', loss='mse')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Wt-HYFMEOB8G"
      },
      "source": [
        "### Execute the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "66JGJt7GOB8H",
        "outputId": "e765ae0f-935b-4ebb-ad1b-fa26add2520e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(train_x, y_train, epochs=100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "800/800 [==============================] - 0s 424us/sample - loss: 5866.3665\n",
            "Epoch 2/100\n",
            "800/800 [==============================] - 0s 29us/sample - loss: 4056.8660\n",
            "Epoch 3/100\n",
            "800/800 [==============================] - 0s 30us/sample - loss: 3817.3034\n",
            "Epoch 4/100\n",
            "800/800 [==============================] - 0s 31us/sample - loss: 3787.5310\n",
            "Epoch 5/100\n",
            "800/800 [==============================] - 0s 29us/sample - loss: 3785.6210\n",
            "Epoch 6/100\n",
            "800/800 [==============================] - 0s 32us/sample - loss: 3785.5769\n",
            "Epoch 7/100\n",
            "800/800 [==============================] - 0s 29us/sample - loss: 3785.0240\n",
            "Epoch 8/100\n",
            "800/800 [==============================] - 0s 32us/sample - loss: 3782.1934\n",
            "Epoch 9/100\n",
            "800/800 [==============================] - 0s 36us/sample - loss: 3783.6998\n",
            "Epoch 10/100\n",
            "800/800 [==============================] - 0s 36us/sample - loss: 3783.6302\n",
            "Epoch 11/100\n",
            "800/800 [==============================] - 0s 34us/sample - loss: 3784.6252\n",
            "Epoch 12/100\n",
            "800/800 [==============================] - 0s 34us/sample - loss: 3786.7925\n",
            "Epoch 13/100\n",
            "800/800 [==============================] - 0s 35us/sample - loss: 3784.7355\n",
            "Epoch 14/100\n",
            "800/800 [==============================] - 0s 37us/sample - loss: 3784.2587\n",
            "Epoch 15/100\n",
            "800/800 [==============================] - 0s 42us/sample - loss: 3784.9288\n",
            "Epoch 16/100\n",
            "800/800 [==============================] - 0s 32us/sample - loss: 3785.2282\n",
            "Epoch 17/100\n",
            "800/800 [==============================] - 0s 32us/sample - loss: 3786.8474\n",
            "Epoch 18/100\n",
            "800/800 [==============================] - 0s 35us/sample - loss: 3784.5632\n",
            "Epoch 19/100\n",
            "800/800 [==============================] - 0s 34us/sample - loss: 3782.5637\n",
            "Epoch 20/100\n",
            "800/800 [==============================] - 0s 35us/sample - loss: 3786.9729\n",
            "Epoch 21/100\n",
            "800/800 [==============================] - 0s 32us/sample - loss: 3784.0296\n",
            "Epoch 22/100\n",
            "800/800 [==============================] - 0s 32us/sample - loss: 3782.9927\n",
            "Epoch 23/100\n",
            "800/800 [==============================] - 0s 39us/sample - loss: 3786.2743\n",
            "Epoch 24/100\n",
            "800/800 [==============================] - 0s 34us/sample - loss: 3784.0797\n",
            "Epoch 25/100\n",
            "800/800 [==============================] - 0s 30us/sample - loss: 3785.2200\n",
            "Epoch 26/100\n",
            "800/800 [==============================] - 0s 32us/sample - loss: 3786.4489\n",
            "Epoch 27/100\n",
            "800/800 [==============================] - 0s 34us/sample - loss: 3784.0954\n",
            "Epoch 28/100\n",
            "800/800 [==============================] - 0s 33us/sample - loss: 3788.0782\n",
            "Epoch 29/100\n",
            "800/800 [==============================] - 0s 33us/sample - loss: 3787.4705\n",
            "Epoch 30/100\n",
            "800/800 [==============================] - 0s 32us/sample - loss: 3784.1580\n",
            "Epoch 31/100\n",
            "800/800 [==============================] - 0s 33us/sample - loss: 3784.1284\n",
            "Epoch 32/100\n",
            "800/800 [==============================] - 0s 35us/sample - loss: 3785.7553\n",
            "Epoch 33/100\n",
            "800/800 [==============================] - 0s 34us/sample - loss: 3783.6113\n",
            "Epoch 34/100\n",
            "800/800 [==============================] - 0s 31us/sample - loss: 3785.5743\n",
            "Epoch 35/100\n",
            "800/800 [==============================] - 0s 30us/sample - loss: 3784.6157\n",
            "Epoch 36/100\n",
            "800/800 [==============================] - 0s 33us/sample - loss: 3783.0909\n",
            "Epoch 37/100\n",
            "800/800 [==============================] - 0s 30us/sample - loss: 3785.4777\n",
            "Epoch 38/100\n",
            "800/800 [==============================] - 0s 30us/sample - loss: 3783.9392\n",
            "Epoch 39/100\n",
            "800/800 [==============================] - 0s 33us/sample - loss: 3784.7312\n",
            "Epoch 40/100\n",
            "800/800 [==============================] - 0s 35us/sample - loss: 3785.0373\n",
            "Epoch 41/100\n",
            "800/800 [==============================] - 0s 34us/sample - loss: 3784.4273\n",
            "Epoch 42/100\n",
            "800/800 [==============================] - 0s 30us/sample - loss: 3786.0947\n",
            "Epoch 43/100\n",
            "800/800 [==============================] - 0s 35us/sample - loss: 3783.4221\n",
            "Epoch 44/100\n",
            "800/800 [==============================] - 0s 32us/sample - loss: 3783.9815\n",
            "Epoch 45/100\n",
            "800/800 [==============================] - 0s 32us/sample - loss: 3784.2984\n",
            "Epoch 46/100\n",
            "800/800 [==============================] - 0s 34us/sample - loss: 3785.2381\n",
            "Epoch 47/100\n",
            "800/800 [==============================] - 0s 35us/sample - loss: 3784.5288\n",
            "Epoch 48/100\n",
            "800/800 [==============================] - 0s 34us/sample - loss: 3783.8233\n",
            "Epoch 49/100\n",
            "800/800 [==============================] - 0s 36us/sample - loss: 3785.9166\n",
            "Epoch 50/100\n",
            "800/800 [==============================] - 0s 32us/sample - loss: 3784.0679\n",
            "Epoch 51/100\n",
            "800/800 [==============================] - 0s 36us/sample - loss: 3783.8793\n",
            "Epoch 52/100\n",
            "800/800 [==============================] - 0s 27us/sample - loss: 3785.1308\n",
            "Epoch 53/100\n",
            "800/800 [==============================] - 0s 32us/sample - loss: 3784.5383\n",
            "Epoch 54/100\n",
            "800/800 [==============================] - 0s 35us/sample - loss: 3784.7755\n",
            "Epoch 55/100\n",
            "800/800 [==============================] - 0s 33us/sample - loss: 3783.4066\n",
            "Epoch 56/100\n",
            "800/800 [==============================] - 0s 37us/sample - loss: 3783.9517\n",
            "Epoch 57/100\n",
            "800/800 [==============================] - 0s 35us/sample - loss: 3784.6633\n",
            "Epoch 58/100\n",
            "800/800 [==============================] - 0s 32us/sample - loss: 3783.7469\n",
            "Epoch 59/100\n",
            "800/800 [==============================] - 0s 37us/sample - loss: 3784.4543\n",
            "Epoch 60/100\n",
            "800/800 [==============================] - 0s 33us/sample - loss: 3785.6775\n",
            "Epoch 61/100\n",
            "800/800 [==============================] - 0s 33us/sample - loss: 3782.9636\n",
            "Epoch 62/100\n",
            "800/800 [==============================] - 0s 31us/sample - loss: 3783.8962\n",
            "Epoch 63/100\n",
            "800/800 [==============================] - 0s 30us/sample - loss: 3784.4168\n",
            "Epoch 64/100\n",
            "800/800 [==============================] - 0s 33us/sample - loss: 3782.6123\n",
            "Epoch 65/100\n",
            "800/800 [==============================] - 0s 31us/sample - loss: 3782.8128\n",
            "Epoch 66/100\n",
            "800/800 [==============================] - 0s 32us/sample - loss: 3783.6715\n",
            "Epoch 67/100\n",
            "800/800 [==============================] - 0s 41us/sample - loss: 3784.5451\n",
            "Epoch 68/100\n",
            "800/800 [==============================] - 0s 35us/sample - loss: 3785.9095\n",
            "Epoch 69/100\n",
            "800/800 [==============================] - 0s 29us/sample - loss: 3785.3224\n",
            "Epoch 70/100\n",
            "800/800 [==============================] - 0s 32us/sample - loss: 3783.9296\n",
            "Epoch 71/100\n",
            "800/800 [==============================] - 0s 32us/sample - loss: 3781.5271\n",
            "Epoch 72/100\n",
            "800/800 [==============================] - 0s 34us/sample - loss: 3781.5775\n",
            "Epoch 73/100\n",
            "800/800 [==============================] - 0s 33us/sample - loss: 3784.4817\n",
            "Epoch 74/100\n",
            "800/800 [==============================] - 0s 42us/sample - loss: 3786.2173\n",
            "Epoch 75/100\n",
            "800/800 [==============================] - 0s 35us/sample - loss: 3783.0487\n",
            "Epoch 76/100\n",
            "800/800 [==============================] - 0s 33us/sample - loss: 3785.6639\n",
            "Epoch 77/100\n",
            "800/800 [==============================] - 0s 35us/sample - loss: 3785.8311\n",
            "Epoch 78/100\n",
            "800/800 [==============================] - 0s 33us/sample - loss: 3785.4966\n",
            "Epoch 79/100\n",
            "800/800 [==============================] - 0s 32us/sample - loss: 3785.8300\n",
            "Epoch 80/100\n",
            "800/800 [==============================] - 0s 30us/sample - loss: 3785.1235\n",
            "Epoch 81/100\n",
            "800/800 [==============================] - 0s 32us/sample - loss: 3784.8147\n",
            "Epoch 82/100\n",
            "800/800 [==============================] - 0s 33us/sample - loss: 3784.0172\n",
            "Epoch 83/100\n",
            "800/800 [==============================] - 0s 30us/sample - loss: 3784.5529\n",
            "Epoch 84/100\n",
            "800/800 [==============================] - 0s 32us/sample - loss: 3783.6025\n",
            "Epoch 85/100\n",
            "800/800 [==============================] - 0s 33us/sample - loss: 3784.2271\n",
            "Epoch 86/100\n",
            "800/800 [==============================] - 0s 35us/sample - loss: 3784.6345\n",
            "Epoch 87/100\n",
            "800/800 [==============================] - 0s 34us/sample - loss: 3783.9005\n",
            "Epoch 88/100\n",
            "800/800 [==============================] - 0s 38us/sample - loss: 3784.4966\n",
            "Epoch 89/100\n",
            "800/800 [==============================] - 0s 33us/sample - loss: 3783.7742\n",
            "Epoch 90/100\n",
            "800/800 [==============================] - 0s 34us/sample - loss: 3785.3458\n",
            "Epoch 91/100\n",
            "800/800 [==============================] - 0s 34us/sample - loss: 3782.6895\n",
            "Epoch 92/100\n",
            "800/800 [==============================] - 0s 29us/sample - loss: 3784.7258\n",
            "Epoch 93/100\n",
            "800/800 [==============================] - 0s 34us/sample - loss: 3783.4845\n",
            "Epoch 94/100\n",
            "800/800 [==============================] - 0s 32us/sample - loss: 3785.3356\n",
            "Epoch 95/100\n",
            "800/800 [==============================] - 0s 37us/sample - loss: 3783.2461\n",
            "Epoch 96/100\n",
            "800/800 [==============================] - 0s 30us/sample - loss: 3784.5801\n",
            "Epoch 97/100\n",
            "800/800 [==============================] - 0s 31us/sample - loss: 3784.9508\n",
            "Epoch 98/100\n",
            "800/800 [==============================] - 0s 31us/sample - loss: 3786.2675\n",
            "Epoch 99/100\n",
            "800/800 [==============================] - 0s 32us/sample - loss: 3785.2454\n",
            "Epoch 100/100\n",
            "800/800 [==============================] - 0s 33us/sample - loss: 3784.5903\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1c79a9aba8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhXzJNN0cYBU",
        "colab_type": "code",
        "outputId": "44f2e395-238e-437e-d105-b8ef0a5562a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 1)                 5         \n",
            "=================================================================\n",
            "Total params: 5\n",
            "Trainable params: 5\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCQwzc6ZcZmE",
        "colab_type": "code",
        "outputId": "f70ae4d7-22ab-43a2-ed72-2aa1dd69b9fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "model.get_weights()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[-0.25881302],\n",
              "        [ 0.04899972],\n",
              "        [ 0.07619543],\n",
              "        [33.563053  ]], dtype=float32), array([34.632206], dtype=float32)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhtWs4oLLO6M",
        "colab_type": "text"
      },
      "source": [
        "### Classification using Keras "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bE5pP3ciLO6O",
        "colab_type": "text"
      },
      "source": [
        "### Load the given Iris data using pandas (Iris.csv)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZY39vBbLO6O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "iris = pd.read_csv('Iris-2.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ary-A7jDLO6P",
        "colab_type": "text"
      },
      "source": [
        "### Splitting the data into feature set and target set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ogmWy-mLO6Q",
        "colab_type": "code",
        "outputId": "45297031-e8a3-4068-8099-0e47e0be3019",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "iris.columns"
      ],
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Id', 'SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm',\n",
              "       'Species'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xNJ5GnTdfM4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X=iris.drop(columns=['Id','Species'])\n",
        "Y=iris.Species"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gpwjgs_tdfXu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2l6k6lQdfiE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#X_train,X_test,y_train,y_test= train_test_split(X,Y,test_size =0.3,random_state = 123)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rR5QF_pdLO6S",
        "colab_type": "text"
      },
      "source": [
        "### Target set has different categories. So, Label encode them. And convert into one-hot vectors using get_dummies in pandas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNRZYAqgfQ9O",
        "colab_type": "code",
        "outputId": "06f1169b-9b9c-4950-85ab-d567f58e23d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "Y.value_counts()"
      ],
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Iris-setosa        50\n",
              "Iris-virginica     50\n",
              "Iris-versicolor    50\n",
              "Name: Species, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 173
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLZr3AKnLO6T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y = pd.get_dummies(Y)\n",
        "#testX = tf.keras.utils.to_categorical(np.array(X_test), num_classes=4)\n",
        "#trainY = tf.keras.utils.to_categorical(y_train, num_classes=4)\n",
        "#testY = tf.keras.utils.to_categorical(y_test, num_classes=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4bLlfQmLO6V",
        "colab_type": "text"
      },
      "source": [
        "### Divide the dataset into Training and test (70:30)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9Ig4ulWLO6W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train,X_test,y_train,y_test= train_test_split(X,Y,test_size =0.3,random_state = 123)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GlbpGYELO6Z",
        "colab_type": "text"
      },
      "source": [
        "### Model\n",
        "Build the model with following layers: <br>\n",
        "1. First dense layer with 10 neurons with input shape 4 (according to the feature set) <br>\n",
        "2. Second Dense layer with 8 neurons <br>\n",
        "3. Output layer with 3 neurons with softmax activation (output layer, 3 neurons as we have 3 classes) <br>\n",
        "4. Use SGD and categorical_crossentropy loss "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsVVUTSyLO6a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Initialize Sequential model\n",
        "model_1 = tf.keras.models.Sequential()\n",
        "\n",
        "#Reshape data from 2D to 1D -> 28x28 to 784\n",
        "#model.add(tf.keras.layers.Re,input_shape=(4,)))\n",
        "\n",
        "#Normalize the data\n",
        "model_1.add(tf.keras.layers.BatchNormalization(input_shape=(4,)))\n",
        "\n",
        "#Add 1st hidden layer\n",
        "model_1.add(tf.keras.layers.Dense(10, activation='sigmoid'))\n",
        "#Add 2nd hidden layer\n",
        "model_1.add(tf.keras.layers.Dense(8, activation='sigmoid'))\n",
        "#Add Dense Layer which provides 10 Outputs after applying softmax\n",
        "model_1.add(tf.keras.layers.Dense(3, activation='softmax'))\n",
        "\n",
        "#Comile the model\n",
        "model_1.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgHR-6MEkD0N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Compile the model\n",
        "model_1.compile(optimizer='sgd', loss='categorical_crossentropy', \n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70LKIEm3LO6b",
        "colab_type": "text"
      },
      "source": [
        "### Fitting the model and predicting "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSJ9pa4ULO6d",
        "colab_type": "code",
        "outputId": "213e6c5d-2009-49b1-a513-34eeae97b572",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model_1.fit(X_train, y_train, \n",
        "          validation_data=(X_test, y_test), \n",
        "          epochs=80,\n",
        "          batch_size=10)"
      ],
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 105 samples, validate on 45 samples\n",
            "Epoch 1/80\n",
            "105/105 [==============================] - 0s 3ms/sample - loss: 1.3577 - acc: 0.3048 - val_loss: 1.1371 - val_acc: 0.4000\n",
            "Epoch 2/80\n",
            "105/105 [==============================] - 0s 256us/sample - loss: 1.2993 - acc: 0.3048 - val_loss: 1.1080 - val_acc: 0.4000\n",
            "Epoch 3/80\n",
            "105/105 [==============================] - 0s 262us/sample - loss: 1.2538 - acc: 0.3048 - val_loss: 1.0863 - val_acc: 0.4000\n",
            "Epoch 4/80\n",
            "105/105 [==============================] - 0s 268us/sample - loss: 1.2168 - acc: 0.3048 - val_loss: 1.0733 - val_acc: 0.4000\n",
            "Epoch 5/80\n",
            "105/105 [==============================] - 0s 265us/sample - loss: 1.1895 - acc: 0.3048 - val_loss: 1.0637 - val_acc: 0.4000\n",
            "Epoch 6/80\n",
            "105/105 [==============================] - 0s 259us/sample - loss: 1.1622 - acc: 0.3048 - val_loss: 1.0583 - val_acc: 0.4000\n",
            "Epoch 7/80\n",
            "105/105 [==============================] - 0s 245us/sample - loss: 1.1440 - acc: 0.3048 - val_loss: 1.0566 - val_acc: 0.4000\n",
            "Epoch 8/80\n",
            "105/105 [==============================] - 0s 267us/sample - loss: 1.1298 - acc: 0.3048 - val_loss: 1.0559 - val_acc: 0.4000\n",
            "Epoch 9/80\n",
            "105/105 [==============================] - 0s 280us/sample - loss: 1.1175 - acc: 0.3048 - val_loss: 1.0568 - val_acc: 0.4000\n",
            "Epoch 10/80\n",
            "105/105 [==============================] - 0s 264us/sample - loss: 1.1092 - acc: 0.3048 - val_loss: 1.0587 - val_acc: 0.4000\n",
            "Epoch 11/80\n",
            "105/105 [==============================] - 0s 265us/sample - loss: 1.1038 - acc: 0.3048 - val_loss: 1.0617 - val_acc: 0.4000\n",
            "Epoch 12/80\n",
            "105/105 [==============================] - 0s 265us/sample - loss: 1.0966 - acc: 0.3048 - val_loss: 1.0643 - val_acc: 0.4000\n",
            "Epoch 13/80\n",
            "105/105 [==============================] - 0s 309us/sample - loss: 1.0912 - acc: 0.3333 - val_loss: 1.0664 - val_acc: 0.4000\n",
            "Epoch 14/80\n",
            "105/105 [==============================] - 0s 287us/sample - loss: 1.0887 - acc: 0.3714 - val_loss: 1.0685 - val_acc: 0.4000\n",
            "Epoch 15/80\n",
            "105/105 [==============================] - 0s 245us/sample - loss: 1.0859 - acc: 0.4095 - val_loss: 1.0707 - val_acc: 0.4222\n",
            "Epoch 16/80\n",
            "105/105 [==============================] - 0s 254us/sample - loss: 1.0832 - acc: 0.5048 - val_loss: 1.0738 - val_acc: 0.4667\n",
            "Epoch 17/80\n",
            "105/105 [==============================] - 0s 343us/sample - loss: 1.0789 - acc: 0.5714 - val_loss: 1.0759 - val_acc: 0.6000\n",
            "Epoch 18/80\n",
            "105/105 [==============================] - 0s 295us/sample - loss: 1.0759 - acc: 0.7048 - val_loss: 1.0778 - val_acc: 0.2222\n",
            "Epoch 19/80\n",
            "105/105 [==============================] - 0s 354us/sample - loss: 1.0759 - acc: 0.4095 - val_loss: 1.0792 - val_acc: 0.2222\n",
            "Epoch 20/80\n",
            "105/105 [==============================] - 0s 311us/sample - loss: 1.0741 - acc: 0.4190 - val_loss: 1.0814 - val_acc: 0.2222\n",
            "Epoch 21/80\n",
            "105/105 [==============================] - 0s 230us/sample - loss: 1.0738 - acc: 0.3905 - val_loss: 1.0815 - val_acc: 0.2222\n",
            "Epoch 22/80\n",
            "105/105 [==============================] - 0s 263us/sample - loss: 1.0716 - acc: 0.3714 - val_loss: 1.0825 - val_acc: 0.2222\n",
            "Epoch 23/80\n",
            "105/105 [==============================] - 0s 243us/sample - loss: 1.0714 - acc: 0.3714 - val_loss: 1.0830 - val_acc: 0.2222\n",
            "Epoch 24/80\n",
            "105/105 [==============================] - 0s 313us/sample - loss: 1.0705 - acc: 0.3810 - val_loss: 1.0833 - val_acc: 0.2222\n",
            "Epoch 25/80\n",
            "105/105 [==============================] - 0s 300us/sample - loss: 1.0697 - acc: 0.3810 - val_loss: 1.0834 - val_acc: 0.2222\n",
            "Epoch 26/80\n",
            "105/105 [==============================] - 0s 277us/sample - loss: 1.0699 - acc: 0.3810 - val_loss: 1.0814 - val_acc: 0.2222\n",
            "Epoch 27/80\n",
            "105/105 [==============================] - 0s 328us/sample - loss: 1.0664 - acc: 0.3810 - val_loss: 1.0815 - val_acc: 0.2222\n",
            "Epoch 28/80\n",
            "105/105 [==============================] - 0s 285us/sample - loss: 1.0678 - acc: 0.3905 - val_loss: 1.0809 - val_acc: 0.2222\n",
            "Epoch 29/80\n",
            "105/105 [==============================] - 0s 330us/sample - loss: 1.0650 - acc: 0.3905 - val_loss: 1.0808 - val_acc: 0.2222\n",
            "Epoch 30/80\n",
            "105/105 [==============================] - 0s 283us/sample - loss: 1.0661 - acc: 0.3810 - val_loss: 1.0814 - val_acc: 0.2222\n",
            "Epoch 31/80\n",
            "105/105 [==============================] - 0s 295us/sample - loss: 1.0635 - acc: 0.3810 - val_loss: 1.0809 - val_acc: 0.2222\n",
            "Epoch 32/80\n",
            "105/105 [==============================] - 0s 260us/sample - loss: 1.0615 - acc: 0.3810 - val_loss: 1.0794 - val_acc: 0.2222\n",
            "Epoch 33/80\n",
            "105/105 [==============================] - 0s 275us/sample - loss: 1.0623 - acc: 0.3810 - val_loss: 1.0814 - val_acc: 0.2222\n",
            "Epoch 34/80\n",
            "105/105 [==============================] - 0s 294us/sample - loss: 1.0621 - acc: 0.3810 - val_loss: 1.0793 - val_acc: 0.2222\n",
            "Epoch 35/80\n",
            "105/105 [==============================] - 0s 304us/sample - loss: 1.0585 - acc: 0.3810 - val_loss: 1.0765 - val_acc: 0.2222\n",
            "Epoch 36/80\n",
            "105/105 [==============================] - 0s 300us/sample - loss: 1.0588 - acc: 0.3810 - val_loss: 1.0757 - val_acc: 0.2222\n",
            "Epoch 37/80\n",
            "105/105 [==============================] - 0s 301us/sample - loss: 1.0576 - acc: 0.3810 - val_loss: 1.0740 - val_acc: 0.2222\n",
            "Epoch 38/80\n",
            "105/105 [==============================] - 0s 309us/sample - loss: 1.0569 - acc: 0.3905 - val_loss: 1.0731 - val_acc: 0.2222\n",
            "Epoch 39/80\n",
            "105/105 [==============================] - 0s 274us/sample - loss: 1.0586 - acc: 0.3810 - val_loss: 1.0723 - val_acc: 0.2222\n",
            "Epoch 40/80\n",
            "105/105 [==============================] - 0s 287us/sample - loss: 1.0547 - acc: 0.3810 - val_loss: 1.0702 - val_acc: 0.2222\n",
            "Epoch 41/80\n",
            "105/105 [==============================] - 0s 285us/sample - loss: 1.0532 - acc: 0.3810 - val_loss: 1.0694 - val_acc: 0.2222\n",
            "Epoch 42/80\n",
            "105/105 [==============================] - 0s 324us/sample - loss: 1.0518 - acc: 0.3810 - val_loss: 1.0663 - val_acc: 0.2222\n",
            "Epoch 43/80\n",
            "105/105 [==============================] - 0s 301us/sample - loss: 1.0544 - acc: 0.3810 - val_loss: 1.0644 - val_acc: 0.2222\n",
            "Epoch 44/80\n",
            "105/105 [==============================] - 0s 301us/sample - loss: 1.0507 - acc: 0.3905 - val_loss: 1.0628 - val_acc: 0.2222\n",
            "Epoch 45/80\n",
            "105/105 [==============================] - 0s 280us/sample - loss: 1.0507 - acc: 0.3810 - val_loss: 1.0608 - val_acc: 0.2222\n",
            "Epoch 46/80\n",
            "105/105 [==============================] - 0s 259us/sample - loss: 1.0508 - acc: 0.3810 - val_loss: 1.0583 - val_acc: 0.2222\n",
            "Epoch 47/80\n",
            "105/105 [==============================] - 0s 273us/sample - loss: 1.0474 - acc: 0.3905 - val_loss: 1.0566 - val_acc: 0.2222\n",
            "Epoch 48/80\n",
            "105/105 [==============================] - 0s 357us/sample - loss: 1.0519 - acc: 0.3714 - val_loss: 1.0565 - val_acc: 0.2222\n",
            "Epoch 49/80\n",
            "105/105 [==============================] - 0s 250us/sample - loss: 1.0445 - acc: 0.3810 - val_loss: 1.0565 - val_acc: 0.2222\n",
            "Epoch 50/80\n",
            "105/105 [==============================] - 0s 350us/sample - loss: 1.0452 - acc: 0.3810 - val_loss: 1.0544 - val_acc: 0.2222\n",
            "Epoch 51/80\n",
            "105/105 [==============================] - 0s 300us/sample - loss: 1.0414 - acc: 0.3810 - val_loss: 1.0534 - val_acc: 0.2222\n",
            "Epoch 52/80\n",
            "105/105 [==============================] - 0s 323us/sample - loss: 1.0427 - acc: 0.3810 - val_loss: 1.0539 - val_acc: 0.2222\n",
            "Epoch 53/80\n",
            "105/105 [==============================] - 0s 243us/sample - loss: 1.0427 - acc: 0.3810 - val_loss: 1.0516 - val_acc: 0.2222\n",
            "Epoch 54/80\n",
            "105/105 [==============================] - 0s 309us/sample - loss: 1.0394 - acc: 0.3810 - val_loss: 1.0493 - val_acc: 0.2222\n",
            "Epoch 55/80\n",
            "105/105 [==============================] - 0s 242us/sample - loss: 1.0478 - acc: 0.3905 - val_loss: 1.0486 - val_acc: 0.2222\n",
            "Epoch 56/80\n",
            "105/105 [==============================] - 0s 256us/sample - loss: 1.0377 - acc: 0.3810 - val_loss: 1.0481 - val_acc: 0.2222\n",
            "Epoch 57/80\n",
            "105/105 [==============================] - 0s 314us/sample - loss: 1.0352 - acc: 0.3810 - val_loss: 1.0453 - val_acc: 0.2222\n",
            "Epoch 58/80\n",
            "105/105 [==============================] - 0s 317us/sample - loss: 1.0359 - acc: 0.3810 - val_loss: 1.0437 - val_acc: 0.2222\n",
            "Epoch 59/80\n",
            "105/105 [==============================] - 0s 271us/sample - loss: 1.0331 - acc: 0.3810 - val_loss: 1.0424 - val_acc: 0.2222\n",
            "Epoch 60/80\n",
            "105/105 [==============================] - 0s 324us/sample - loss: 1.0382 - acc: 0.3714 - val_loss: 1.0408 - val_acc: 0.2222\n",
            "Epoch 61/80\n",
            "105/105 [==============================] - 0s 421us/sample - loss: 1.0331 - acc: 0.3810 - val_loss: 1.0379 - val_acc: 0.2222\n",
            "Epoch 62/80\n",
            "105/105 [==============================] - 0s 263us/sample - loss: 1.0418 - acc: 0.3905 - val_loss: 1.0355 - val_acc: 0.2222\n",
            "Epoch 63/80\n",
            "105/105 [==============================] - 0s 286us/sample - loss: 1.0319 - acc: 0.4095 - val_loss: 1.0335 - val_acc: 0.2222\n",
            "Epoch 64/80\n",
            "105/105 [==============================] - 0s 263us/sample - loss: 1.0292 - acc: 0.4000 - val_loss: 1.0309 - val_acc: 0.2222\n",
            "Epoch 65/80\n",
            "105/105 [==============================] - 0s 295us/sample - loss: 1.0286 - acc: 0.4190 - val_loss: 1.0290 - val_acc: 0.2222\n",
            "Epoch 66/80\n",
            "105/105 [==============================] - 0s 274us/sample - loss: 1.0247 - acc: 0.3714 - val_loss: 1.0265 - val_acc: 0.2889\n",
            "Epoch 67/80\n",
            "105/105 [==============================] - 0s 340us/sample - loss: 1.0254 - acc: 0.4476 - val_loss: 1.0258 - val_acc: 0.2222\n",
            "Epoch 68/80\n",
            "105/105 [==============================] - 0s 280us/sample - loss: 1.0262 - acc: 0.4000 - val_loss: 1.0232 - val_acc: 0.2222\n",
            "Epoch 69/80\n",
            "105/105 [==============================] - 0s 286us/sample - loss: 1.0268 - acc: 0.4190 - val_loss: 1.0214 - val_acc: 0.2667\n",
            "Epoch 70/80\n",
            "105/105 [==============================] - 0s 322us/sample - loss: 1.0210 - acc: 0.4190 - val_loss: 1.0189 - val_acc: 0.2222\n",
            "Epoch 71/80\n",
            "105/105 [==============================] - 0s 283us/sample - loss: 1.0186 - acc: 0.3905 - val_loss: 1.0164 - val_acc: 0.2222\n",
            "Epoch 72/80\n",
            "105/105 [==============================] - 0s 300us/sample - loss: 1.0210 - acc: 0.4190 - val_loss: 1.0141 - val_acc: 0.2889\n",
            "Epoch 73/80\n",
            "105/105 [==============================] - 0s 353us/sample - loss: 1.0125 - acc: 0.4190 - val_loss: 1.0141 - val_acc: 0.2222\n",
            "Epoch 74/80\n",
            "105/105 [==============================] - 0s 309us/sample - loss: 1.0121 - acc: 0.3905 - val_loss: 1.0111 - val_acc: 0.2889\n",
            "Epoch 75/80\n",
            "105/105 [==============================] - 0s 281us/sample - loss: 1.0106 - acc: 0.4286 - val_loss: 1.0102 - val_acc: 0.2444\n",
            "Epoch 76/80\n",
            "105/105 [==============================] - 0s 367us/sample - loss: 1.0094 - acc: 0.4381 - val_loss: 1.0081 - val_acc: 0.2222\n",
            "Epoch 77/80\n",
            "105/105 [==============================] - 0s 294us/sample - loss: 1.0049 - acc: 0.4190 - val_loss: 1.0052 - val_acc: 0.2444\n",
            "Epoch 78/80\n",
            "105/105 [==============================] - 0s 264us/sample - loss: 1.0048 - acc: 0.4095 - val_loss: 1.0030 - val_acc: 0.2667\n",
            "Epoch 79/80\n",
            "105/105 [==============================] - 0s 254us/sample - loss: 1.0029 - acc: 0.4476 - val_loss: 0.9999 - val_acc: 0.3111\n",
            "Epoch 80/80\n",
            "105/105 [==============================] - 0s 288us/sample - loss: 1.0044 - acc: 0.4952 - val_loss: 0.9987 - val_acc: 0.2889\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1c75558ef0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 178
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMl7tQ3pLO6f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10tHGSsGLO6h",
        "colab_type": "text"
      },
      "source": [
        "### Report Accuracy of the predicted values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4vJbpVNLO6h",
        "colab_type": "code",
        "outputId": "657c0a27-4bb8-456d-9e63-10258a2062f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 773
        }
      },
      "source": [
        "pred =model_1.predict(X_test[:])\n",
        "pred"
      ],
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.26817566, 0.37562233, 0.35620204],\n",
              "       [0.25741825, 0.37541258, 0.3671691 ],\n",
              "       [0.25303438, 0.37288526, 0.37408033],\n",
              "       [0.30456686, 0.38050726, 0.31492594],\n",
              "       [0.36951968, 0.37730986, 0.25317037],\n",
              "       [0.27679297, 0.37752327, 0.34568378],\n",
              "       [0.27060318, 0.3746029 , 0.35479397],\n",
              "       [0.3718404 , 0.37654606, 0.25161353],\n",
              "       [0.3656201 , 0.37599304, 0.25838688],\n",
              "       [0.2922586 , 0.3789992 , 0.32874212],\n",
              "       [0.25470617, 0.37358597, 0.37170786],\n",
              "       [0.37971437, 0.38049236, 0.23979326],\n",
              "       [0.29500002, 0.37688154, 0.32811844],\n",
              "       [0.2690577 , 0.3787504 , 0.35219193],\n",
              "       [0.24760327, 0.37452766, 0.37786904],\n",
              "       [0.2565731 , 0.3751754 , 0.36825153],\n",
              "       [0.3707905 , 0.37863427, 0.25057527],\n",
              "       [0.37147546, 0.3755748 , 0.25294974],\n",
              "       [0.28680044, 0.38033146, 0.3328681 ],\n",
              "       [0.3620373 , 0.37682986, 0.26113284],\n",
              "       [0.37101898, 0.37820992, 0.2507712 ],\n",
              "       [0.2762115 , 0.37914243, 0.34464613],\n",
              "       [0.37330493, 0.37837538, 0.24831972],\n",
              "       [0.2751605 , 0.37769613, 0.3471434 ],\n",
              "       [0.37776908, 0.37932184, 0.24290907],\n",
              "       [0.36932012, 0.37900436, 0.25167555],\n",
              "       [0.36373186, 0.37868264, 0.25758556],\n",
              "       [0.2683456 , 0.37409732, 0.35755706],\n",
              "       [0.2666101 , 0.38358724, 0.34980267],\n",
              "       [0.37549964, 0.3784446 , 0.24605575],\n",
              "       [0.25544584, 0.3763937 , 0.36816055],\n",
              "       [0.26366702, 0.37308425, 0.3632487 ],\n",
              "       [0.37016523, 0.37738198, 0.25245282],\n",
              "       [0.37343618, 0.3783048 , 0.24825896],\n",
              "       [0.2816144 , 0.3760632 , 0.3423224 ],\n",
              "       [0.2998215 , 0.37593588, 0.32424265],\n",
              "       [0.27187046, 0.38495627, 0.3431733 ],\n",
              "       [0.37264287, 0.38024095, 0.24711622],\n",
              "       [0.3724054 , 0.37967578, 0.24791877],\n",
              "       [0.29380432, 0.37866476, 0.32753095],\n",
              "       [0.28784996, 0.37418187, 0.3379681 ],\n",
              "       [0.36844647, 0.3772072 , 0.25434643],\n",
              "       [0.25633633, 0.37610966, 0.36755395],\n",
              "       [0.25350404, 0.37287456, 0.37362134],\n",
              "       [0.26530287, 0.3744251 , 0.3602721 ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcUYGgREkOFP",
        "colab_type": "code",
        "outputId": "904d3029-1ef5-42ec-aaf2-9fbe010aa2f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pred[3]"
      ],
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.30456686, 0.38050726, 0.31492594], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 182
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DxJohq_kWOD",
        "colab_type": "code",
        "outputId": "d2624d54-90be-445c-c6b0-0c0d8dc1d063",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "model_1.summary()"
      ],
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "batch_normalization_9 (Batch (None, 4)                 16        \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 10)                50        \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 8)                 88        \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 3)                 27        \n",
            "=================================================================\n",
            "Total params: 181\n",
            "Trainable params: 173\n",
            "Non-trainable params: 8\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xvucz4eik3xo",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOarD2w_lDvr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "092bd46a-2f95-44b0-cace-a008a2a583df"
      },
      "source": [
        "model_1.evaluate(X_test,y_test)"
      ],
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "45/45 [==============================] - 0s 2ms/sample - loss: 0.9987 - acc: 0.2889\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.998675278822581, 0.2888889]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 185
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2wX-zgqufES",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkap-thxxP5-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}