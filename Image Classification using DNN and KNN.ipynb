{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 79
    },
    "colab_type": "code",
    "id": "B3r1aM-6BPGo",
    "outputId": "2286d66d-272e-4ec9-c606-73e17d6cf438"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ib-DNA6STHzo"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from scipy.spatial.distance import cdist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wj-9Ia_eclCo"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "MDaD--bwBdP-",
    "outputId": "09d8c3b1-7816-4d95-d75a-258465d6fb78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z3TinbFgBeO4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2GGrlHsGNNwv"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from scipy.spatial.distance import cdist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8GqJE5W7CtrD"
   },
   "outputs": [],
   "source": [
    "f1 = h5py.File(\"/content/drive/My Drive/Colab folder/SVHN_single_grey1.h5\",'r')\n",
    "#print(list(f1.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "wKKjrxmdHHKy",
    "outputId": "4a8ade5d-52a8-4c09-c83f-308f90469227"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['X_test', 'X_train', 'X_val', 'y_test', 'y_train', 'y_val']\n"
     ]
    }
   ],
   "source": [
    "print(list(f1.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YrtPI9BFHjt3"
   },
   "outputs": [],
   "source": [
    "X_train = np.array(f1['X_train'])\n",
    "X_test = np.array(f1['X_test'])\n",
    "X_val = np.array(f1['X_val'])\n",
    "y_test = np.array(f1['y_test'])\n",
    "y_train = np.array(f1['y_train'])\n",
    "y_val = np.array(f1['y_val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "22i3eLStgPYX",
    "outputId": "a6c035c3-8171-4afe-8b6a-f5a6be193402"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 32, 32) (18000, 32, 32) (60000, 32, 32) (42000,) (18000,) (60000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape,X_test.shape,X_val.shape,y_train.shape,y_test.shape,y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TywNDX70ALRW"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZZ58llqu4ObO"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "a3A4LFbZ53m7",
    "outputId": "cc2984ca-ccd6-4058-8d3a-1f294b4481f0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 32, 32)"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "60WEMGeW8qRf"
   },
   "outputs": [],
   "source": [
    "#nxtrainsamples, nxtrain1, nytrain1 = X_train.shape\n",
    "#X_train_new = X_train.reshape((nxtrainsamples,nxtrain1*nytrain1))\n",
    "#nxtestsamples, nxtest1, nytest1 = X_test.shape\n",
    "#X_test_new = X_test.reshape((nxtestsamples,28*28))\n",
    "\n",
    "#nytainsamples, nxtrain2, nytrain2 = y_train.shape\n",
    "#y_train_new = y_train.reshape((nytrainsamples,nxtrain2*nytrain2))\n",
    "#nytestsamples, nxtest2, nytest2 = y_test.shape\n",
    "#y_test_new = y_test.reshape((nytestsamples,nxtest2*nytest2))\n",
    "\n",
    "#nvalsxsamples, nxtrain3, nytrain3 = X_val.shape\n",
    "#X_val_new = X_val.reshape((nvalsxsamples,28*28))\n",
    "#nvalsysamples, nxtest3, nytest3 = y_val.shape\n",
    "#y_val_new = y_val.reshape((nvalsysamples,nxtest3*nytest3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a48gQkGs5_z4"
   },
   "outputs": [],
   "source": [
    "nxtrainsamples, nxtrain1, nytrain1 = X_train.shape\n",
    "X_train_new = X_train.reshape((nxtrainsamples,nxtrain1*nytrain1))\n",
    "nxtestsamples, nxtest1, nytest1 = X_test.shape\n",
    "X_test_new = X_test.reshape((nxtestsamples,nxtest1*nytest1))\n",
    "\n",
    "#nytainsamples, nxtrain2, nytrain2 = y_train.shape\n",
    "#y_train_new = y_train.reshape((nytrainsamples,nxtrain2*nytrain2))\n",
    "#nytestsamples, nxtest2, nytest2 = y_test.shape\n",
    "#y_test_new = y_test.reshape((nytestsamples,nxtest2*nytest2))\n",
    "\n",
    "nvalsxsamples, nxtrain3, nytrain3 = X_val.shape\n",
    "X_val_new = X_val.reshape((nvalsxsamples,nxtrain3*nytrain3))\n",
    "#nvalsysamples, nxtest3, nytest3 = y_val.shape\n",
    "#y_val_new = y_val.reshape((nvalsysamples,nxtest3*nytest3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "dBM_XfWR3jDQ",
    "outputId": "27bbff7c-9471-4fb5-c856-c3e2b9c9747b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 1024)\n"
     ]
    }
   ],
   "source": [
    "print(X_val_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "9XbKWlmC6zI8",
    "outputId": "36964d4a-bef8-4714-e838-2b1668f9e0b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 28, 28)\n",
      "(18000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "#Create a resized dataset for training and testing inputs with corresponding size. Here we are resizing it to 28X28 (same input size as MNIST)\n",
    "x_train_res = np.zeros((X_train.shape[0],28,28), dtype=np.float32)\n",
    "for i in range(X_train.shape[0]):\n",
    "  #using cv2.resize to resize each train example to 28X28 size using Cubic interpolation\n",
    "  x_train_res[i,:,:] = cv2.resize(X_train[i], dsize=(28, 28), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "x_test_res = np.zeros((X_test.shape[0],28,28), dtype=np.float32)\n",
    "for i in range(X_test.shape[0]):\n",
    "  #using cv2.resize to resize each test example to 28X28 size using Cubic interpolation\n",
    "  x_test_res[i,:,:] = cv2.resize(X_test[i], dsize=(28, 28), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "x_val_res = np.zeros((X_val.shape[0],28,28), dtype=np.float32)\n",
    "for i in range(X_val.shape[0]):\n",
    "  #using cv2.resize to resize each test example to 28X28 size using Cubic interpolation\n",
    "  x_val_res[i,:,:] = cv2.resize(X_val[i], dsize=(28, 28), interpolation=cv2.INTER_CUBIC)\n",
    "  \n",
    "#We don't need the original dataset anynmore so we can clear up memory consumed by original dataset\n",
    "#del x_train\n",
    "#del x_test\n",
    "\n",
    "print(x_train_res.shape)\n",
    "print(x_test_res.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "gsITTdq32DQt",
    "outputId": "9b98bf8a-e538-4ab0-e82a-e920a67eb2df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.52      0.49      1814\n",
      "           1       0.50      0.57      0.53      1828\n",
      "           2       0.54      0.48      0.51      1803\n",
      "           3       0.37      0.35      0.36      1719\n",
      "           4       0.60      0.57      0.59      1812\n",
      "           5       0.38      0.33      0.36      1768\n",
      "           6       0.38      0.39      0.39      1832\n",
      "           7       0.63      0.59      0.61      1808\n",
      "           8       0.34      0.37      0.36      1812\n",
      "           9       0.39      0.40      0.40      1804\n",
      "\n",
      "    accuracy                           0.46     18000\n",
      "   macro avg       0.46      0.46      0.46     18000\n",
      "weighted avg       0.46      0.46      0.46     18000\n",
      "\n",
      "1 0.45916666666666667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.68      0.46      1814\n",
      "           1       0.40      0.71      0.52      1828\n",
      "           2       0.49      0.54      0.51      1803\n",
      "           3       0.35      0.38      0.36      1719\n",
      "           4       0.60      0.56      0.58      1812\n",
      "           5       0.45      0.32      0.37      1768\n",
      "           6       0.48      0.31      0.38      1832\n",
      "           7       0.78      0.55      0.64      1808\n",
      "           8       0.45      0.25      0.32      1812\n",
      "           9       0.60      0.32      0.41      1804\n",
      "\n",
      "    accuracy                           0.46     18000\n",
      "   macro avg       0.49      0.46      0.46     18000\n",
      "weighted avg       0.50      0.46      0.46     18000\n",
      "\n",
      "3 0.4617777777777778\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.66      0.48      1814\n",
      "           1       0.44      0.70      0.54      1828\n",
      "           2       0.55      0.52      0.53      1803\n",
      "           3       0.42      0.39      0.41      1719\n",
      "           4       0.67      0.61      0.64      1812\n",
      "           5       0.48      0.36      0.41      1768\n",
      "           6       0.47      0.38      0.42      1832\n",
      "           7       0.72      0.58      0.64      1808\n",
      "           8       0.42      0.33      0.37      1812\n",
      "           9       0.52      0.36      0.43      1804\n",
      "\n",
      "    accuracy                           0.49     18000\n",
      "   macro avg       0.51      0.49      0.49     18000\n",
      "weighted avg       0.51      0.49      0.49     18000\n",
      "\n",
      "5 0.49016666666666664\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.67      0.52      1814\n",
      "           1       0.46      0.72      0.56      1828\n",
      "           2       0.56      0.54      0.55      1803\n",
      "           3       0.40      0.41      0.41      1719\n",
      "           4       0.65      0.63      0.64      1812\n",
      "           5       0.47      0.38      0.42      1768\n",
      "           6       0.48      0.41      0.44      1832\n",
      "           7       0.72      0.60      0.66      1808\n",
      "           8       0.44      0.33      0.37      1812\n",
      "           9       0.57      0.38      0.46      1804\n",
      "\n",
      "    accuracy                           0.51     18000\n",
      "   macro avg       0.52      0.51      0.50     18000\n",
      "weighted avg       0.52      0.51      0.50     18000\n",
      "\n",
      "7 0.5070555555555556\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.68      0.52      1814\n",
      "           1       0.46      0.73      0.56      1828\n",
      "           2       0.57      0.55      0.56      1803\n",
      "           3       0.42      0.41      0.42      1719\n",
      "           4       0.64      0.63      0.64      1812\n",
      "           5       0.49      0.38      0.43      1768\n",
      "           6       0.49      0.40      0.44      1832\n",
      "           7       0.73      0.60      0.66      1808\n",
      "           8       0.45      0.34      0.39      1812\n",
      "           9       0.56      0.39      0.46      1804\n",
      "\n",
      "    accuracy                           0.51     18000\n",
      "   macro avg       0.52      0.51      0.51     18000\n",
      "weighted avg       0.52      0.51      0.51     18000\n",
      "\n",
      "9 0.5124444444444445\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,10,2):\n",
    "  knn = KNeighborsClassifier(n_neighbors=i)\n",
    "  modelknn = knn.fit(X_train_new,y_train)\n",
    "  predknn = knn.predict(X_test_new)\n",
    "  w= metrics.accuracy_score(predknn,y_test)\n",
    "  print(classification_report(y_test, predknn))\n",
    "  print (i,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z-NTQEH4SuT0"
   },
   "outputs": [],
   "source": [
    "#y_pred_new = knn.predict(X_test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "colab_type": "code",
    "id": "35HUdeYrSqF3",
    "outputId": "bd7a62d3-f3cc-4a7d-8230-9c35fec69087"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.68      0.52      1814\n",
      "           1       0.46      0.73      0.56      1828\n",
      "           2       0.57      0.55      0.56      1803\n",
      "           3       0.42      0.41      0.42      1719\n",
      "           4       0.64      0.63      0.64      1812\n",
      "           5       0.49      0.38      0.43      1768\n",
      "           6       0.49      0.40      0.44      1832\n",
      "           7       0.73      0.60      0.66      1808\n",
      "           8       0.45      0.34      0.39      1812\n",
      "           9       0.56      0.39      0.46      1804\n",
      "\n",
      "    accuracy                           0.51     18000\n",
      "   macro avg       0.52      0.51      0.51     18000\n",
      "weighted avg       0.52      0.51      0.51     18000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, predknn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "GSuOOzrII2vL",
    "outputId": "45a085f1-b6f7-4073-9658-8ab25fd78c33"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "distortions = []\n",
    "K = range(1,15)\n",
    "for k in K:\n",
    "    kmeanModel = KMeans(n_clusters=k)\n",
    "    kmeanModel.fit(X_train_new)\n",
    "    distortions.append(sum(np.min(cdist(X_train_new, kmeanModel.cluster_centers_, 'euclidean'), axis=1)) / X_train_new.shape[0])\n",
    "\n",
    "# Plot the elbow\n",
    "plt.plot(K, distortions, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('The Elbow Method showing the optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3nFcVKKeT6RK"
   },
   "outputs": [],
   "source": [
    "kmean_y_pred = kmeanModel.predict(X_test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "colab_type": "code",
    "id": "fN7XdGYMn0Xi",
    "outputId": "1007e23f-c006-45de-a928-e71107ffc017"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.15      0.07      0.10      1814\n",
      "           1       0.14      0.05      0.07      1828\n",
      "           2       0.16      0.04      0.07      1803\n",
      "           3       0.09      0.10      0.10      1719\n",
      "           4       0.10      0.06      0.07      1812\n",
      "           5       0.11      0.12      0.11      1768\n",
      "           6       0.09      0.09      0.09      1832\n",
      "           7       0.11      0.11      0.11      1808\n",
      "           8       0.10      0.08      0.09      1812\n",
      "           9       0.10      0.11      0.11      1804\n",
      "          10       0.00      0.00      0.00         0\n",
      "          11       0.00      0.00      0.00         0\n",
      "          12       0.00      0.00      0.00         0\n",
      "          13       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.08     18000\n",
      "   macro avg       0.08      0.06      0.07     18000\n",
      "weighted avg       0.12      0.08      0.09     18000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, kmean_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tu3BeJYURjmB"
   },
   "source": [
    "from the above elbow curve optimal K is 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9-OdpnUoZZsF"
   },
   "outputs": [],
   "source": [
    "trainY = tf.keras.utils.to_categorical(y_train, num_classes=10)\n",
    "testY = tf.keras.utils.to_categorical(y_test, num_classes=10)\n",
    "valy = tf.keras.utils.to_categorical(y_val, num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LuU0EqwDK0HM"
   },
   "source": [
    "model without Batch normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OCC9YsSYchwj"
   },
   "outputs": [],
   "source": [
    "#Initialize Sequential model\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "#Reshape data from 2D to 1D -> 28x28 to 784\n",
    "model.add(tf.keras.layers.Reshape((784,),input_shape=(28,28,)))\n",
    "\n",
    "#Normalize the data\n",
    "#model.add(tf.keras.layers.BatchNormalization())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0AxTs7Cm46r3"
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Reshape((784,),input_shape=(28,28,)))\n",
    "#model.add(tf.keras.layers.BatchNormalization())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "ujtP3pxOFh8H",
    "outputId": "f4bc29b1-7479-42c1-d8cc-9c4079eac24b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "#Hidden layers\n",
    "model.add(tf.keras.layers.Dense(200, activation='relu', name='Layer_1'))\n",
    "\n",
    "#model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "model.add(tf.keras.layers.Dense(100, activation='relu', name='Layer_2'))\n",
    "#model.add(tf.keras.layers.BatchNormalization())\n",
    "#Dropout layer\n",
    "#model.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "#Hidden layers\n",
    "model.add(tf.keras.layers.Dense(60, activation='relu', name='Layer_3'))\n",
    "#model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dense(30, activation='relu', name='Layer_4'))\n",
    "\n",
    "#Dropout layer\n",
    "model.add(tf.keras.layers.Dropout(0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a0pkhMJpFnW0"
   },
   "outputs": [],
   "source": [
    "#Output layer\n",
    "model.add(tf.keras.layers.Dense(10, activation='softmax', name='Output'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WFbLD-03FpiV"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "niKuHv0S0YVU"
   },
   "outputs": [],
   "source": [
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_acc', patience=10, verbose=1, mode='auto')\n",
    "callback_list = [early_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "phLP46_qIBZd"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "nrs5hWjt0YS7",
    "outputId": "7a19f476-dfca-4fae-be53-96477d7f8151"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1 µs, total: 4 µs\n",
      "Wall time: 8.58 µs\n",
      "Train on 42000 samples, validate on 60000 samples\n",
      "Epoch 1/96\n",
      "42000/42000 [==============================] - 3s 71us/sample - loss: nan - acc: 0.0995 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 2/96\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 3/96\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 4/96\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 5/96\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 6/96\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 7/96\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 8/96\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 9/96\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 10/96\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 11/96\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 12/96\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 13/96\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 14/96\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 15/96\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 16/96\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 17/96\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 18/96\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 19/96\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 20/96\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 21/96\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 22/96\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 23/96\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 24/96\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 25/96\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 26/96\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 27/96\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 28/96\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 29/96\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 30/96\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 31/96\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 32/96\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 33/96\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 34/96\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 35/96\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 36/96\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 37/96\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 38/96\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 39/96\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 40/96\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 41/96\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 42/96\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 43/96\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 44/96\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 45/96\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 46/96\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 47/96\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 48/96\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 49/96\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 50/96\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 51/96\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 52/96\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 53/96\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 54/96\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 55/96\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 56/96\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 57/96\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 58/96\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 59/96\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 60/96\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 61/96\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 62/96\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 63/96\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 64/96\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 65/96\n",
      "42000/42000 [==============================] - 3s 65us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 66/96\n",
      "42000/42000 [==============================] - 3s 65us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 67/96\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 68/96\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 69/96\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 70/96\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 71/96\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 72/96\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 73/96\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 74/96\n",
      "42000/42000 [==============================] - 3s 65us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 75/96\n",
      "42000/42000 [==============================] - 3s 65us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 76/96\n",
      "42000/42000 [==============================] - 3s 65us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 77/96\n",
      "42000/42000 [==============================] - 3s 63us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 78/96\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 79/96\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 80/96\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 81/96\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 82/96\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 83/96\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 84/96\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 85/96\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 86/96\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 87/96\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 88/96\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 89/96\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 90/96\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 91/96\n",
      "42000/42000 [==============================] - 3s 63us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 92/96\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 93/96\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 94/96\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 95/96\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n",
      "Epoch 96/96\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: nan - acc: 0.0997 - val_loss: nan - val_acc: 0.1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f2d45b96d68>"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time\n",
    "#Training on the dataset\n",
    "model.fit(x_train_res, trainY,\n",
    "          batch_size=batch_size,\n",
    "          epochs=96,\n",
    "          verbose=1,\n",
    "          validation_data=(x_val_res, valy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 386
    },
    "colab_type": "code",
    "id": "z7ZJAlaFFq2n",
    "outputId": "ae185397-3c56-40cf-eb60-b7be29db2c2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape (Reshape)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "Layer_1 (Dense)              (None, 200)               157000    \n",
      "_________________________________________________________________\n",
      "Layer_2 (Dense)              (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "Layer_3 (Dense)              (None, 60)                6060      \n",
      "_________________________________________________________________\n",
      "Layer_4 (Dense)              (None, 30)                1830      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                310       \n",
      "=================================================================\n",
      "Total params: 185,300\n",
      "Trainable params: 185,300\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "HDYT04GDKTen",
    "outputId": "565ef5bb-879e-4b26-c7bc-71939a24ddae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 37,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C3QZbEItK42A"
   },
   "source": [
    "model with batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_dF_wijPFsR2"
   },
   "outputs": [],
   "source": [
    "#Initialize Sequential model\n",
    "model_2 = tf.keras.models.Sequential()\n",
    "\n",
    "#Reshape data from 2D to 1D -> 28x28 to 784\n",
    "model_2.add(tf.keras.layers.Reshape((784,),input_shape=(28,28,)))\n",
    "\n",
    "#Normalize the data\n",
    "model_2.add(tf.keras.layers.BatchNormalization())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F924jMFKLBBz"
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model_2 = tf.keras.models.Sequential()\n",
    "model_2.add(tf.keras.layers.Reshape((784,),input_shape=(28,28,)))\n",
    "model_2.add(tf.keras.layers.BatchNormalization())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3mSi-oc_LIRR"
   },
   "outputs": [],
   "source": [
    "#Hidden layers\n",
    "model_2.add(tf.keras.layers.Dense(200, activation='relu', name='Layer_1'))\n",
    "\n",
    "model_2.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "model_2.add(tf.keras.layers.Dense(100, activation='relu', name='Layer_2'))\n",
    "model_2.add(tf.keras.layers.BatchNormalization())\n",
    "#Dropout layer\n",
    "model_2.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "#Hidden layers\n",
    "model_2.add(tf.keras.layers.Dense(60, activation='relu', name='Layer_3'))\n",
    "model_2.add(tf.keras.layers.BatchNormalization())\n",
    "model_2.add(tf.keras.layers.Dense(30, activation='relu', name='Layer_4'))\n",
    "\n",
    "#Dropout layer\n",
    "model_2.add(tf.keras.layers.Dropout(0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ub8_2e1ALSn4"
   },
   "outputs": [],
   "source": [
    "#Output layer\n",
    "model_2.add(tf.keras.layers.Dense(10, activation='softmax', name='Output'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mz1wRZYTLVva"
   },
   "outputs": [],
   "source": [
    "model_2.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "fu1TWCJv9m4l",
    "outputId": "ac9087c0-2277-4049-c5cb-8b2e01ae5207"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 0 ns, total: 5 µs\n",
      "Wall time: 10 µs\n",
      "Train on 42000 samples, validate on 60000 samples\n",
      "Epoch 1/96\n",
      "42000/42000 [==============================] - 5s 126us/sample - loss: 2.4945 - acc: 0.1280 - val_loss: 2.1566 - val_acc: 0.2315\n",
      "Epoch 2/96\n",
      "42000/42000 [==============================] - 4s 106us/sample - loss: 2.2286 - acc: 0.1906 - val_loss: 1.9694 - val_acc: 0.3414\n",
      "Epoch 3/96\n",
      "42000/42000 [==============================] - 4s 106us/sample - loss: 2.0732 - acc: 0.2517 - val_loss: 1.7809 - val_acc: 0.4383\n",
      "Epoch 4/96\n",
      "42000/42000 [==============================] - 4s 105us/sample - loss: 1.9073 - acc: 0.3206 - val_loss: 1.5940 - val_acc: 0.5124\n",
      "Epoch 5/96\n",
      "42000/42000 [==============================] - 4s 106us/sample - loss: 1.7657 - acc: 0.3755 - val_loss: 1.4551 - val_acc: 0.5522\n",
      "Epoch 6/96\n",
      "42000/42000 [==============================] - 5s 109us/sample - loss: 1.6461 - acc: 0.4252 - val_loss: 1.3469 - val_acc: 0.5839\n",
      "Epoch 7/96\n",
      "42000/42000 [==============================] - 4s 106us/sample - loss: 1.5478 - acc: 0.4672 - val_loss: 1.2382 - val_acc: 0.6145\n",
      "Epoch 8/96\n",
      "42000/42000 [==============================] - 4s 106us/sample - loss: 1.4703 - acc: 0.4978 - val_loss: 1.1578 - val_acc: 0.6465\n",
      "Epoch 9/96\n",
      "42000/42000 [==============================] - 4s 106us/sample - loss: 1.4063 - acc: 0.5229 - val_loss: 1.1745 - val_acc: 0.6301\n",
      "Epoch 10/96\n",
      "42000/42000 [==============================] - 5s 107us/sample - loss: 1.3489 - acc: 0.5457 - val_loss: 1.0514 - val_acc: 0.6741\n",
      "Epoch 11/96\n",
      "42000/42000 [==============================] - 4s 105us/sample - loss: 1.3013 - acc: 0.5667 - val_loss: 1.1552 - val_acc: 0.6277\n",
      "Epoch 12/96\n",
      "42000/42000 [==============================] - 5s 108us/sample - loss: 1.2562 - acc: 0.5848 - val_loss: 1.7883 - val_acc: 0.5181\n",
      "Epoch 13/96\n",
      "42000/42000 [==============================] - 4s 105us/sample - loss: 1.2245 - acc: 0.5953 - val_loss: 0.9352 - val_acc: 0.7113\n",
      "Epoch 14/96\n",
      "42000/42000 [==============================] - 5s 110us/sample - loss: 1.1792 - acc: 0.6147 - val_loss: 1.1806 - val_acc: 0.6358\n",
      "Epoch 15/96\n",
      "42000/42000 [==============================] - 4s 106us/sample - loss: 1.1507 - acc: 0.6262 - val_loss: 1.1592 - val_acc: 0.6382\n",
      "Epoch 16/96\n",
      "42000/42000 [==============================] - 5s 121us/sample - loss: 1.1121 - acc: 0.6383 - val_loss: 1.8905 - val_acc: 0.5178\n",
      "Epoch 17/96\n",
      "42000/42000 [==============================] - 5s 108us/sample - loss: 1.0872 - acc: 0.6506 - val_loss: 1.0182 - val_acc: 0.6815\n",
      "Epoch 18/96\n",
      "42000/42000 [==============================] - 4s 107us/sample - loss: 1.0569 - acc: 0.6636 - val_loss: 0.8492 - val_acc: 0.7361\n",
      "Epoch 19/96\n",
      "42000/42000 [==============================] - 5s 108us/sample - loss: 1.0282 - acc: 0.6724 - val_loss: 0.8241 - val_acc: 0.7407\n",
      "Epoch 20/96\n",
      "42000/42000 [==============================] - 4s 107us/sample - loss: 1.0065 - acc: 0.6804 - val_loss: 0.7737 - val_acc: 0.7587\n",
      "Epoch 21/96\n",
      "42000/42000 [==============================] - 4s 106us/sample - loss: 0.9838 - acc: 0.6899 - val_loss: 1.2261 - val_acc: 0.6519\n",
      "Epoch 22/96\n",
      "42000/42000 [==============================] - 4s 106us/sample - loss: 0.9697 - acc: 0.6936 - val_loss: 0.7976 - val_acc: 0.7494\n",
      "Epoch 23/96\n",
      "42000/42000 [==============================] - 5s 107us/sample - loss: 0.9543 - acc: 0.7031 - val_loss: 0.8402 - val_acc: 0.7352\n",
      "Epoch 24/96\n",
      "42000/42000 [==============================] - 4s 107us/sample - loss: 0.9361 - acc: 0.7071 - val_loss: 0.6813 - val_acc: 0.7911\n",
      "Epoch 25/96\n",
      "42000/42000 [==============================] - 5s 107us/sample - loss: 0.9135 - acc: 0.7146 - val_loss: 0.7356 - val_acc: 0.7668\n",
      "Epoch 26/96\n",
      "42000/42000 [==============================] - 4s 107us/sample - loss: 0.9092 - acc: 0.7180 - val_loss: 0.8555 - val_acc: 0.7389\n",
      "Epoch 27/96\n",
      "42000/42000 [==============================] - 4s 106us/sample - loss: 0.8923 - acc: 0.7253 - val_loss: 1.9954 - val_acc: 0.5672\n",
      "Epoch 28/96\n",
      "42000/42000 [==============================] - 5s 107us/sample - loss: 0.8889 - acc: 0.7248 - val_loss: 1.0017 - val_acc: 0.7031\n",
      "Epoch 29/96\n",
      "42000/42000 [==============================] - 4s 107us/sample - loss: 0.8657 - acc: 0.7339 - val_loss: 0.6323 - val_acc: 0.8067\n",
      "Epoch 30/96\n",
      "42000/42000 [==============================] - 4s 107us/sample - loss: 0.8421 - acc: 0.7406 - val_loss: 0.8465 - val_acc: 0.7418\n",
      "Epoch 31/96\n",
      "42000/42000 [==============================] - 4s 107us/sample - loss: 0.8368 - acc: 0.7410 - val_loss: 0.8964 - val_acc: 0.7323\n",
      "Epoch 32/96\n",
      "42000/42000 [==============================] - 4s 106us/sample - loss: 0.8374 - acc: 0.7438 - val_loss: 0.7796 - val_acc: 0.7621\n",
      "Epoch 33/96\n",
      "42000/42000 [==============================] - 5s 110us/sample - loss: 0.8217 - acc: 0.7481 - val_loss: 0.8683 - val_acc: 0.7350\n",
      "Epoch 34/96\n",
      "42000/42000 [==============================] - 4s 107us/sample - loss: 0.8141 - acc: 0.7500 - val_loss: 0.7263 - val_acc: 0.7729\n",
      "Epoch 35/96\n",
      "42000/42000 [==============================] - 5s 108us/sample - loss: 0.7971 - acc: 0.7573 - val_loss: 0.7290 - val_acc: 0.7706\n",
      "Epoch 36/96\n",
      "42000/42000 [==============================] - 4s 106us/sample - loss: 0.7849 - acc: 0.7613 - val_loss: 0.6014 - val_acc: 0.8140\n",
      "Epoch 37/96\n",
      "42000/42000 [==============================] - 5s 107us/sample - loss: 0.7741 - acc: 0.7625 - val_loss: 1.4057 - val_acc: 0.6742\n",
      "Epoch 38/96\n",
      "42000/42000 [==============================] - 4s 106us/sample - loss: 0.7775 - acc: 0.7623 - val_loss: 0.6181 - val_acc: 0.8076\n",
      "Epoch 39/96\n",
      "42000/42000 [==============================] - 4s 107us/sample - loss: 0.7572 - acc: 0.7693 - val_loss: 1.0307 - val_acc: 0.7146\n",
      "Epoch 40/96\n",
      "42000/42000 [==============================] - 4s 107us/sample - loss: 0.7599 - acc: 0.7680 - val_loss: 0.6813 - val_acc: 0.7905\n",
      "Epoch 41/96\n",
      "42000/42000 [==============================] - 5s 108us/sample - loss: 0.7443 - acc: 0.7745 - val_loss: 1.1851 - val_acc: 0.6910\n",
      "Epoch 42/96\n",
      "42000/42000 [==============================] - 4s 106us/sample - loss: 0.7520 - acc: 0.7732 - val_loss: 1.3706 - val_acc: 0.6778\n",
      "Epoch 43/96\n",
      "42000/42000 [==============================] - 4s 106us/sample - loss: 0.7357 - acc: 0.7778 - val_loss: 1.3077 - val_acc: 0.6792\n",
      "Epoch 44/96\n",
      "42000/42000 [==============================] - 5s 107us/sample - loss: 0.7321 - acc: 0.7769 - val_loss: 0.8645 - val_acc: 0.7424\n",
      "Epoch 45/96\n",
      "42000/42000 [==============================] - 5s 108us/sample - loss: 0.7222 - acc: 0.7803 - val_loss: 0.5519 - val_acc: 0.8331\n",
      "Epoch 46/96\n",
      "42000/42000 [==============================] - 5s 108us/sample - loss: 0.7103 - acc: 0.7857 - val_loss: 0.6131 - val_acc: 0.8108\n",
      "Epoch 47/96\n",
      "42000/42000 [==============================] - 5s 109us/sample - loss: 0.6978 - acc: 0.7905 - val_loss: 2.1473 - val_acc: 0.5704\n",
      "Epoch 48/96\n",
      "42000/42000 [==============================] - 4s 105us/sample - loss: 0.7125 - acc: 0.7854 - val_loss: 0.8398 - val_acc: 0.7496\n",
      "Epoch 49/96\n",
      "42000/42000 [==============================] - 4s 105us/sample - loss: 0.7021 - acc: 0.7876 - val_loss: 0.7678 - val_acc: 0.7734\n",
      "Epoch 50/96\n",
      "42000/42000 [==============================] - 4s 107us/sample - loss: 0.6921 - acc: 0.7902 - val_loss: 0.8921 - val_acc: 0.7452\n",
      "Epoch 51/96\n",
      "42000/42000 [==============================] - 4s 105us/sample - loss: 0.6783 - acc: 0.7952 - val_loss: 0.5079 - val_acc: 0.8430\n",
      "Epoch 52/96\n",
      "42000/42000 [==============================] - 4s 106us/sample - loss: 0.6727 - acc: 0.7954 - val_loss: 0.5961 - val_acc: 0.8174\n",
      "Epoch 53/96\n",
      "42000/42000 [==============================] - 4s 106us/sample - loss: 0.6758 - acc: 0.7970 - val_loss: 1.4111 - val_acc: 0.6728\n",
      "Epoch 54/96\n",
      "42000/42000 [==============================] - 5s 107us/sample - loss: 0.6683 - acc: 0.7984 - val_loss: 0.5275 - val_acc: 0.8363\n",
      "Epoch 55/96\n",
      "42000/42000 [==============================] - 5s 107us/sample - loss: 0.6602 - acc: 0.8015 - val_loss: 0.9090 - val_acc: 0.7503\n",
      "Epoch 56/96\n",
      "42000/42000 [==============================] - 5s 115us/sample - loss: 0.6624 - acc: 0.8010 - val_loss: 0.6530 - val_acc: 0.8063\n",
      "Epoch 57/96\n",
      "42000/42000 [==============================] - 4s 106us/sample - loss: 0.6507 - acc: 0.8055 - val_loss: 0.7665 - val_acc: 0.7703\n",
      "Epoch 58/96\n",
      "42000/42000 [==============================] - 4s 106us/sample - loss: 0.6453 - acc: 0.8058 - val_loss: 0.4996 - val_acc: 0.8449\n",
      "Epoch 59/96\n",
      "42000/42000 [==============================] - 4s 106us/sample - loss: 0.6460 - acc: 0.8066 - val_loss: 0.7141 - val_acc: 0.7997\n",
      "Epoch 60/96\n",
      "42000/42000 [==============================] - 4s 105us/sample - loss: 0.6417 - acc: 0.8083 - val_loss: 0.5825 - val_acc: 0.8182\n",
      "Epoch 61/96\n",
      "42000/42000 [==============================] - 5s 108us/sample - loss: 0.6363 - acc: 0.8096 - val_loss: 0.4474 - val_acc: 0.8633\n",
      "Epoch 62/96\n",
      "42000/42000 [==============================] - 5s 108us/sample - loss: 0.6292 - acc: 0.8118 - val_loss: 0.5282 - val_acc: 0.8355\n",
      "Epoch 63/96\n",
      "42000/42000 [==============================] - 5s 107us/sample - loss: 0.6181 - acc: 0.8148 - val_loss: 0.6106 - val_acc: 0.8154\n",
      "Epoch 64/96\n",
      "42000/42000 [==============================] - 4s 105us/sample - loss: 0.6221 - acc: 0.8143 - val_loss: 1.0224 - val_acc: 0.7333\n",
      "Epoch 65/96\n",
      "42000/42000 [==============================] - 5s 113us/sample - loss: 0.6260 - acc: 0.8140 - val_loss: 0.5776 - val_acc: 0.8312\n",
      "Epoch 66/96\n",
      "42000/42000 [==============================] - 5s 111us/sample - loss: 0.6128 - acc: 0.8177 - val_loss: 0.7042 - val_acc: 0.7888\n",
      "Epoch 67/96\n",
      "42000/42000 [==============================] - 5s 108us/sample - loss: 0.6089 - acc: 0.8191 - val_loss: 0.8156 - val_acc: 0.7639\n",
      "Epoch 68/96\n",
      "42000/42000 [==============================] - 4s 105us/sample - loss: 0.6041 - acc: 0.8184 - val_loss: 1.2091 - val_acc: 0.6982\n",
      "Epoch 69/96\n",
      "42000/42000 [==============================] - 4s 107us/sample - loss: 0.6126 - acc: 0.8182 - val_loss: 0.7726 - val_acc: 0.7830\n",
      "Epoch 70/96\n",
      "42000/42000 [==============================] - 5s 109us/sample - loss: 0.6039 - acc: 0.8207 - val_loss: 0.6764 - val_acc: 0.7925\n",
      "Epoch 71/96\n",
      "42000/42000 [==============================] - 4s 106us/sample - loss: 0.5922 - acc: 0.8238 - val_loss: 1.6717 - val_acc: 0.6542\n",
      "Epoch 72/96\n",
      "42000/42000 [==============================] - 4s 107us/sample - loss: 0.6022 - acc: 0.8228 - val_loss: 0.8492 - val_acc: 0.7649\n",
      "Epoch 73/96\n",
      "42000/42000 [==============================] - 4s 107us/sample - loss: 0.5950 - acc: 0.8227 - val_loss: 0.6599 - val_acc: 0.8061\n",
      "Epoch 74/96\n",
      "42000/42000 [==============================] - 4s 106us/sample - loss: 0.5926 - acc: 0.8246 - val_loss: 0.4325 - val_acc: 0.8693\n",
      "Epoch 75/96\n",
      "42000/42000 [==============================] - 4s 105us/sample - loss: 0.5751 - acc: 0.8292 - val_loss: 0.4467 - val_acc: 0.8640\n",
      "Epoch 76/96\n",
      "42000/42000 [==============================] - 4s 106us/sample - loss: 0.5746 - acc: 0.8305 - val_loss: 0.5275 - val_acc: 0.8407\n",
      "Epoch 77/96\n",
      "42000/42000 [==============================] - 4s 105us/sample - loss: 0.5728 - acc: 0.8274 - val_loss: 0.5925 - val_acc: 0.8249\n",
      "Epoch 78/96\n",
      "42000/42000 [==============================] - 4s 106us/sample - loss: 0.5740 - acc: 0.8304 - val_loss: 0.9217 - val_acc: 0.7619\n",
      "Epoch 79/96\n",
      "42000/42000 [==============================] - 4s 106us/sample - loss: 0.5820 - acc: 0.8272 - val_loss: 0.6098 - val_acc: 0.8174\n",
      "Epoch 80/96\n",
      "42000/42000 [==============================] - 4s 104us/sample - loss: 0.5705 - acc: 0.8317 - val_loss: 0.6311 - val_acc: 0.8282\n",
      "Epoch 81/96\n",
      "42000/42000 [==============================] - 4s 107us/sample - loss: 0.5663 - acc: 0.8318 - val_loss: 0.5265 - val_acc: 0.8399\n",
      "Epoch 82/96\n",
      "42000/42000 [==============================] - 4s 105us/sample - loss: 0.5636 - acc: 0.8322 - val_loss: 0.5698 - val_acc: 0.8263\n",
      "Epoch 83/96\n",
      "42000/42000 [==============================] - 4s 106us/sample - loss: 0.5577 - acc: 0.8361 - val_loss: 0.5173 - val_acc: 0.8407\n",
      "Epoch 84/96\n",
      "42000/42000 [==============================] - 4s 106us/sample - loss: 0.5543 - acc: 0.8349 - val_loss: 0.7385 - val_acc: 0.7996\n",
      "Epoch 85/96\n",
      "42000/42000 [==============================] - 4s 107us/sample - loss: 0.5537 - acc: 0.8337 - val_loss: 0.6276 - val_acc: 0.8222\n",
      "Epoch 86/96\n",
      "42000/42000 [==============================] - 5s 108us/sample - loss: 0.5552 - acc: 0.8362 - val_loss: 0.9396 - val_acc: 0.7549\n",
      "Epoch 87/96\n",
      "42000/42000 [==============================] - 4s 106us/sample - loss: 0.5561 - acc: 0.8352 - val_loss: 0.7009 - val_acc: 0.8006\n",
      "Epoch 88/96\n",
      "42000/42000 [==============================] - 4s 107us/sample - loss: 0.5498 - acc: 0.8379 - val_loss: 1.3287 - val_acc: 0.6874\n",
      "Epoch 89/96\n",
      "42000/42000 [==============================] - 5s 108us/sample - loss: 0.5550 - acc: 0.8370 - val_loss: 0.5246 - val_acc: 0.8372\n",
      "Epoch 90/96\n",
      "42000/42000 [==============================] - 4s 107us/sample - loss: 0.5339 - acc: 0.8412 - val_loss: 0.4840 - val_acc: 0.8518\n",
      "Epoch 91/96\n",
      "42000/42000 [==============================] - 4s 106us/sample - loss: 0.5441 - acc: 0.8389 - val_loss: 1.1133 - val_acc: 0.7190\n",
      "Epoch 92/96\n",
      "42000/42000 [==============================] - 4s 107us/sample - loss: 0.5589 - acc: 0.8352 - val_loss: 0.4388 - val_acc: 0.8685\n",
      "Epoch 93/96\n",
      "42000/42000 [==============================] - 4s 106us/sample - loss: 0.5439 - acc: 0.8400 - val_loss: 0.4439 - val_acc: 0.8653\n",
      "Epoch 94/96\n",
      "42000/42000 [==============================] - 4s 106us/sample - loss: 0.5349 - acc: 0.8419 - val_loss: 0.8550 - val_acc: 0.7769\n",
      "Epoch 95/96\n",
      "42000/42000 [==============================] - 4s 106us/sample - loss: 0.5344 - acc: 0.8415 - val_loss: 0.6082 - val_acc: 0.8272\n",
      "Epoch 96/96\n",
      "42000/42000 [==============================] - 5s 108us/sample - loss: 0.5389 - acc: 0.8390 - val_loss: 0.4020 - val_acc: 0.8781\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f2d35ebceb8>"
      ]
     },
     "execution_count": 48,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time\n",
    "#Training on the dataset\n",
    "model_2.fit(x_train_res, trainY,\n",
    "          batch_size=batch_size,\n",
    "          epochs=96,\n",
    "          verbose=1,\n",
    "          validation_data=(x_val_res, valy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 554
    },
    "colab_type": "code",
    "id": "lQmnKy94LXQW",
    "outputId": "95b3f565-55a1-42ef-e137-fecd978aff17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape (Reshape)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 784)               3136      \n",
      "_________________________________________________________________\n",
      "Layer_1 (Dense)              (None, 200)               157000    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 200)               800       \n",
      "_________________________________________________________________\n",
      "Layer_2 (Dense)              (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "Layer_3 (Dense)              (None, 60)                6060      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 60)                240       \n",
      "_________________________________________________________________\n",
      "Layer_4 (Dense)              (None, 30)                1830      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                310       \n",
      "=================================================================\n",
      "Total params: 189,876\n",
      "Trainable params: 187,588\n",
      "Non-trainable params: 2,288\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fPyQywTAw8lE"
   },
   "source": [
    "Based on our experimentation of various classification mechanisms both classical as well through Deep Neural Networks, we may reach the conclusion that DNNs are quite efficient for image classification.\n",
    "\n",
    "Metrics show accuracy to be sub par i.e. below 50% for classical ML KNN models whereas they show a healthy accuracy for tuned models around 84%"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
